# RNN-Transformer-NLP-Scratch
# RNNs and Transformers from Scratch: Sequence Modeling & Attention

This project implements the evolution of sequence modeling, from basic Recurrent Neural Networks to gated architectures and modern Attention mechanisms.

## üß† Architectures Implemented
* **Recurrent Cells**: From-scratch implementations of Vanilla RNN, LSTM, and GRU units.
* **Attention Mechanisms**: Scaled dot-product attention and Multi-Head Self-Attention modules.
* **Transformers**: Includes positional encoding and core transformer components.

## üöÄ Key Applications
* **Text Generation**: Character-level language modeling trained on Shakespearean text.
* **Sentiment Analysis**: A Bidirectional LSTM (BiLSTM) classifier with an attention-based aggregation layer.

## üõ†Ô∏è Tech Stack
* **Language**: Python 3
* **Libraries**: NumPy, Matplotlib.
